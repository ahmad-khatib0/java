
# To inject an Istio proxy manually into the Pods of an existing Deployment object
$ kubectl get deployment sample-deployment -o yaml | istioctl kube-inject -f - | kubectl apply -f -
+ The istioctl kube-inject command reads the definition from the kubectl get deployment command and adds an 
  extra container for an Istio proxy in Pods that the Deployment handles. The configuration for the existing 
  container in the Deployment object is updated so that incoming and outgoing traffic goes through the Istio proxy.
+ The istioctl command returns the new definition of the Deployment object, 
  including a container for the Istio proxy.

## Deploying Istio in a Kubernetes cluster
# Run a precheck to verify that the Kubernetes cluster is ready for Istio to be installed in it:
$ istioctl experimental precheck
# Install Istio using the demo profile with: 
$ istioctl install --skip-confirmation --set profile=demo --set meshConfig.accessLogFile=/dev/stdout \
    --set meshConfig.accessLogEncoding=JSON --set values.pilot.env.PILOT_JWT_PUB_KEY_REFRESH_INTERVAL=15s \
    -f kubernetes/istio-tracing.yml
# Wait for the Deployment objects and their Pods to be available with the following command:
$ kubectl -n istio-system wait --timeout=600s --for=condition=available deployment --all

# Next, install the extra components Kiali, Jaeger, Prometheus, and Grafana – with these commands:
$ istio_version=$(istioctl version --short --remote=false)
- echo "Installing integrations for Istio v$istio_version"
$ kubectl apply -n istio-system -f https://raw.githubusercontent.com/istio/istio/${istio_version}/samples/addons/kiali.yaml
$ kubectl apply -n istio-system -f https://raw.githubusercontent.com/istio/istio/${istio_version}/samples/addons/jaeger.yaml
$ kubectl apply -n istio-system -f https://raw.githubusercontent.com/istio/istio/${istio_version}/samples/addons/prometheus.yaml
$ kubectl apply -n istio-system -f https://raw.githubusercontent.com/istio/istio/${istio_version}/samples/addons/grafana.yaml
$ kubectl -n istio-system get deploy

# Setting up access to Istio services
# apply the Helm chart:
$ helm upgrade --install istio-hands-on-addons kubernetes/helm/environments/istio-system -n istio-system --wait
++ This will result in the gateway being able to route requests for the following hostnames:
   • kiali.minikube.me requests are routed to kiali:20001
   • tracing.minikube.me requests are routed to tracing:80
   • prometheus.minikube.me requests are routed to prometheus:9000
   • grafana.minikube.me requests are routed to grafana:3000

# To verify that the certificate and secret objects have been created
$ kubectl -n istio-system get secret hands-on-certificate
$ kubectl -n istio-system get certificate hands-on-certificate
# Perform the following steps to set up the Minikube tunnel and register the hostnames: 
# Run the following command in a separate terminal window: 
$ minikube tunnel
# Configure the hostnames to be resolved to the IP address of the Istio ingress gateway.
$ INGRESS_IP=$(kubectl -n istio-system get service istio-ingressgateway \
  -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
# Update /etc/hosts so that all minikube.me hostnames will use the IP address of the Istio ingress gateway:
$ MINIKUBE_HOSTS="minikube.me grafana.minikube.me kiali.minikube.me prometheus.minikube.me 
  tracing.minikube.me kibana.minikube.me elasticsearch.minikube.me mail.minikube.me health.minikube.me"
$ echo 127.0.0.1 $MINIKUBE_HOSTS" | sudo tee -a /etc/hosts

# Verify that Kiali, Jaeger, Grafana, and Prometheus can be reached through the tunnel 
$ curl -o /dev/null -sk -L -w "%{http_code}\n" https://kiali.minikube.me/kiali/
$ curl -o /dev/null -sk -L -w "%{http_code}\n" https://tracing.minikube.me
$ curl -o /dev/null -sk -L -w "%{http_code}\n" https://grafana.minikube.me
$ curl -o /dev/null -sk -L -w "%{http_code}\n" https://prometheus.minikube.me/graph#/

# Running commands to create the service mesh
$ eval $(minikube docker-env -u)
$ ./gradlew build
$ eval $(minikube docker-env)
$ docker-compose build
# Recreate the hands-on Namespace, and set it as the default Namespace:
$ kubectl delete namespace hands-on
$ kubectl apply -f kubernetes/hands-on-namespace.yml
$ kubectl config set-context $(kubectl config current-context) --namespace=hands-on
# Resolve the Helm chart dependencies with the following commands:
$ for f in kubernetes/helm/components/*; do helm dep up $f; done
$ for f in kubernetes/helm/environments/*; do helm dep up $f; done
$ helm install hands-on-dev-env kubernetes/helm/environments/dev-env -n hands-on --wait
$ kubectl get pods
$ ./test-em-all.bash

# Logging propagation of trace and span IDs
$ kubectl edit cm product-composite
$ Uncomment the two lines that are found in ./kubernetes/helm/components/product-composite/config-repo/product-composite.yml
$ kubectl delete pod -l app=product-composite       # restart 
$ kubectl logs -f -l app=product-composite
# Acquire an access token and make a request using the access token:
$ unset ACCESS_TOKEN
$ ACCESS_TOKEN=$(curl -k https://writer:secret-writer@minikube.me/oauth2/token 
  -d grant_type=client_credentials -d scope="product:read product:write" -s | jq -r .access_token)
$ echo $ACCESS_TOKEN
$ curl -H "Authorization: Bearer $ACCESS_TOKEN" -k https://minikube.me/product-composite/1 -w "%{http_code}\n" -o /dev/null -s
$ Revert to not logging trace and span IDs by adding back the comments in 
  the ConfigMap and restart the microservice by deleting its Pod.


## Observing the service mesh
$ ACCESS_TOKEN=$(curl https://writer:secret-writer@minikube.me/oauth2/token 
  -d grant_type=client_credentials -d scope="product:read product:write" -ks | jq .access_token -r)
$ siege https://minikube.me/product-composite/1 -H "Authorization: Bearer $ACCESS_TOKEN" -c1 -d1 -v
$ Open Kiali’s web UI:  https://kiali.minikube.me  and look at the requests sent between microservices
$ Open Jaeger web UI:   https://tracing.minikube.me  to see  Distributed traces 


## Authenticating external requests using OAuth 2.0/OIDC access tokens
$ ACCESS_TOKEN=$(curl https://writer:secret-writer@minikube.me/oauth2/token 
  -d grant_type=client_credentials -d scope="product:read product:write" -ks | jq .access_token -r)
$ echo ACCESS_TOKEN=$ACCESS_TOKEN
$ curl -k https://minikube.me/product-composite/1 -H "Authorization: Bearer $ACCESS_TOKEN" -i
$ Edit the RequestAuthentication object and temporarily change the issuer, 
  for example, to http://auth-server-x (this intended to see if istio doing its job for validating tokens)
# Verify the change:
$ kubectl get RequestAuthentication product-composite-request-authentication -o yaml 
# Make the request again. It should fail with the HTTP response status code 401 (Unauthorized)
  and the error message Jwt issuer is not configured:
$ curl -k https://minikube.me/product-composite/1 -H "Authorization: Bearer $ACCESS_TOKEN" -i
++ Since it takes a few seconds for Istio to propagate the change you might want to repeat the request
# Revert the changed name of the issuer to http://auth-server:
$ kubectl edit RequestAuthentication product-composite-request-authentication
# Verify that the request works again. First, wait a few seconds for the change to be propagated. Then
$ curl -k https://minikube.me/product-composite/1 -H "Authorization: Bearer $ACCESS_TOKEN"


## Testing resilience by injecting faults
$ Ensure that the load tests using siege are running.
$ kubectl apply -f kubernetes/resilience-tests/product-virtual-service-with-faults.yml
$ Monitor the output from the siege load tests tool.
$ kubectl delete -f kubernetes/resilience-tests/product-virtual-service-with-faults.yml

## Testing resilience by injecting delays
# Perform the following steps to test the circuit breaker by injecting a delay:
$ kubectl apply -f kubernetes/resilience-tests/product-virtual-service-with-delay.yml
$ ACCESS_TOKEN=$(curl https://writer:secret-writer@minikube.me/oauth2/token 
  -d grant_type=client_credentials -d scope="product:read product:write" -ks | jq .access_token -r)
$ echo ACCESS_TOKEN=$ACCESS_TOKEN
$ for i in {1..6}; do time curl -k https://minikube.me/product-composite/1 -H "Authorization: Bearer $ACCESS_TOKEN"; done
++ Expect the following:
a. The circuit opens up after the first three failed calls
b. The circuit breaker applies fast-fail logic for the last three calls
c. A fallback response is returned for the last three calls

# Simulate the delay problem being fixed by removing the temporary delay with the following command:
$ kubectl delete -f kubernetes/resilience-tests/product-virtual-service-with-delay.yml
++ Verify that correct state returned again, and without any delay, by sending a new request using the for loop 


####
  Finally, to be able to deploy multiple versions of the three core microservices, their Helm charts have
  been duplicated in the kubernetes/helm/components folder. The name of the new charts is suffixed
  with -green. The only difference compared to the existing charts is that they don’t include the Service
  template from the common chart, avoiding the creation of two Service objects per core microservice.
  The new charts are named product-green, recommendation-green, and review-green.

  

## Deploying v1 and v2 versions of the microservices with routing to the v1 version
$ helm uninstall hands-on-dev-env  # (so we can deploy the v1 and v2 versions)
# Start MySQL, MongoDB, and RabbitMQ outside of Kubernetes:
$ eval $(minikube docker-env)
$ docker-compose up -d mongodb mysql rabbitmq

# Tag the Docker images with v1 and v2 versions:
$ docker tag hands-on/auth-server hands-on/auth-server:v1
$ docker tag hands-on/product-composite-service hands-on/product-composite-service:v1
$ docker tag hands-on/product-service hands-on/product-service:v1
$ docker tag hands-on/recommendation-service hands-on/recommendation-service:v1
$ docker tag hands-on/review-service hands-on/review-service:v1
$ docker tag hands-on/product-service hands-on/product-service:v2
$ docker tag hands-on/recommendation-service hands-on/recommendation-service:v2
$ docker tag hands-on/review-service hands-on/review-service:v2

# Deploy the system landscape using Helm and wait for all deployments to complete:
$ helm install hands-on-prod-env kubernetes/helm/environments/prod-env -n hands-on --wait
# Once the dep completed, verify that we have v1 and v2 Pods up and running for the three core microservices 
$ kubectl get pods
$ ./test-em-all.bash
++ Unfortunately, the tests will fail initially with an error message like:
   Response Body: Jwks doesn't have key to match kid or alg from Jwt
   This error is caused by the Istio daemon, istiod, caching the JWKS public keys from the auth
   server in the development environment. The auth server in the production environment will
   have new JWKS keys but the same identity as istiod, so it tries to reuse the old JWKS public
   keys, causing this failure. Istio caches JWKS public keys for 20 minutes by default, but when
   installing Istio, we lowered the refresh interval to 15 seconds; see the Deploying Istio in a Kubernetes 
   cluster section. So, after waiting a short while, up to a minute depending on how quickly
   the refreshed keys are propagated, you should be able to run the tests successfully. 


## Verifying that all traffic initially goes to the v1 version of the microservices
$ ACCESS_TOKEN=$(curl https://writer:secret-writer@minikube.me/oauth2/token 
  -d grant_type=client_credentials -d scope="product:read product:write" -ks | jq .access_token -r)
$ echo ACCESS_TOKEN=$ACCESS_TOKEN
$ siege https://minikube.me/product-composite/1 -H "Authorization: Bearer $ACCESS_TOKEN" -c1 -d1 -v
$ go to Kiali’s web UI (https://kiali.minikube.me) and visualize from where the request is comming

# Running canary tests
$ ACCESS_TOKEN=$(curl https://writer:secret-writer@minikube.me/oauth2/token
   -d grant_type=client_credentials -d scope="product:read product:write" -ks | jq .access_token -r)
$ echo ACCESS_TOKEN=$ACCESS_TOKEN
# the serviceAddresses To see which version of a microservice served a request
$ curl -ks https://minikube.me/product-composite/1 -H "Authorization: Bearer $ACCESS_TOKEN" | jq .serviceAddresses
# If we add the X-group=test header (match header), we expect the request to be served by v2 versions 
$ curl -ks https://minikube.me/product-composite/1 -H "Authorization: Bearer $ACCESS_TOKEN" -H "X-group: test" | jq .serviceAddresses



## Running a blue-green deployment (using patch without changing manifisit files)
# change the weight distribution of the routing to the v1 and v2 Pods in the review microservice
$ kubectl patch virtualservice review --type=json -p='[
    {"op": "add", "path": "/spec/http/1/route/0/weight", "value": 80},
    {"op": "add", "path": "/spec/http/1/route/1/weight", "value": 20}
]'
++ The command will configure the routing rules of the review microservice to route 80% of the requests
   to the old version, and 20% of the requests to the new version.
   To specify that the weight value should be changed in the review virtual service, the /spec/http/1/
   route/0/weight path is given for the old version, and /spec/http/1/route/1/weight for the new version.
   The 0 and 1 in the path are used to specify the index of array elements in the definition of the virtual
   service. For example, http/1 means the second element in the array under the http element. See the
   definition of the preceding review virtual service.
   From the definition, we can see that the first element with index 0 is the match element, which we will
   not change. The second element is the route element, which we want to change.
$ go to the Kiali web UI, check Traffic Distribution, observe the percentage between v1 and v2 

# if you want to route all traffic to the v2 version of all microservices: 
$ ./kubernetes/routing-tests/split-traffic-between-old-and-new-services.bash 0 100
# If something goes terribly wrong following the upgrade to v2, so downgrade 
$ ./kubernetes/routing-tests/split-traffic-between-old-and-new-services.bash 100 0



## Running tests with Docker Compose (to ensure that source code dose not dependent on a platform like k8s)
$ USE_K8S=false HOST=localhost PORT=8443 HEALTH_URL=https://localhost:8443 ./test-em-all.bash start stop

