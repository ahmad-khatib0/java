-- When we say a log is ordered, what we mean is that a record’s position in the log is fixed, and never changes

-- Kafka topics are extremely flexible with what you store in them. For example, you can have homgenous 
   topics that contain only one type of data, or heterogeneous topics that contain multiple types of data.

-- A bootstrap server is the host/IP pair for one or more brokers

-- Multiple consumer groups can consume from a single topic, and each consumer 
     group processes messages independently of other consumer groups
     
-- While partitions can be added to an existing topic, the recommended pattern is to create a new source topic
     with the desired number of partitions, and to migrate all of the existing workloads to the new topic.


There are 3 basic kinds of processors in Kafka Streams:
Source processors - 
    Sources are where information flows into the Kafka Streams application.
    Data is read from a Kafka topic and sent to one or more stream processors.
  
Stream processors - 
    These processors are responsible for applying data processing / transformation logic
    on the input stream. In the high-level DSL, these processors are defined 
    using a set of built-in operators that are exposed by the Kafka Streams 
    library, Some example operators are: filter, map, flatMap, and join.

Sink processors - 
    Sinks are where enriched, transformed, filtered, or otherwise processed records are written 
    back to Kafka, either to be handled by another stream processing application or to be sent 
    to a downstream datastore via something like Kafka Connect. Like source processors, 
    sink processors are connected to a Kafka topic.


-- if your topology reads from one source topic that contains 16 partitions,
    then Kafka Streams will create 16 tasks, each of which will instantiate it’s own copy
    of the underlying processor topology. Once Kafka Streams has created all of the
    tasks, it will assign the source partitions to be read from to each task.


# init a project with gradle
$ gradle init \
    --type java-application \
    --dsl groovy \
    --test-framework junit-jupiter \
    --project-name my-project \
    --package com.example


-- Kafka Streams leverages a programming paradigm called dataflow programming (DFP), which is a 
      data-centric method of representing programs as a series of inputs, outputs, and processing 
      stages. This leads to a very natural and intuitive way of creating stream processing programs...
      
      Instead of building a program as a sequence of steps, the stream processing logic in a
      Kafka Streams application is structured as a directed acyclic graph (DAG), where nodes
      represent a processing step, or processor, and the edges represent input
      and output streams (where data flows from one processor to another)


--  There are two ways to model the data in your Kafka topics: 
        as a Stream (also called a record stream) 
        or a Table (also known as a changelog stream)


-- KStream: an abstraction of a partitioned record stream, in which data is represented 
      using insert semantics (i.e. each event is considered to be independent of other events).

-- KTable: an abstraction of a partitioned table (i.e. changelog stream), in which data is 
      represented using update semantics (the latest representation of a given key is
      tracked by the application). Since KTables are partitioned,
      each Kafka Streams task contains only a subset of the full table.
      
GlobalKTable: similar to a KTable, except each GlobalKTable contains a
      complete (i.e. unpartitioned) copy of the underlying data. 


 +-----------------------------+
 | -- Kafka Stream Conclusion: |
 +-----------------------------+

-- Kafka Streams lives in the stream processing layer of the Kafka ecosystem.
      This is where sophisticated data processing, transformation, and enrichment happens.

-- Kafka Streams was built to simplify the development of stream processing
      applications with a simple, functional API and a set of stream processing
      primitives that can be reused across projects. When more control is needed, a
      lower-level Processor API can also be used to define your topology.

-- Kafka Streams has a friendlier learning curve and a simpler deployment
      model than cluster-based solutions like Apache Flink and Apache Spark
      Streaming. It also supports event-at-a-time processing, which is considered true streaming.

-- Kafka Streams is great for solving problems that require or benefit from realtime decision-making
      and data processing. Furthermore, it is reliable, main- tainable, scalable, and elastic.





-- The simplest form of stream processing requires no memory of previously seen events. Each event 
      is consumed, processed1, and subsequently forgotten. This par- adigm is called stateless processing,

 +------------------------------------------------------------------------------------+
 | -- The distinction between stateless and stateful stream processing applications : |
 +------------------------------------------------------------------------------------+
-- In stateless applications, each event handled by your Kafka Streams application is processed 
      independently of other events, and only Stream views are needed by your 
      application In other words, your application treats each event as a self-contained 
      insert and requires no mem- ory of previously seen events.

-- Stateful applications, on the other hand, need to remember information about
      previously seen events in one or more steps of your processor topology, usually for the 
      purpose of aggregating, windowing, or joining event streams. These applications are more 
      complex under the hood since they need to track additional data, or state.


.. Some operators, like filter, are considered stateless because they only need to look at the 
      current record to perform an action (in this case, filter looks at each record individually 
      to determine whether or not the record should be forwarded to downstream processors).
      Other operators, like count, are stateful since they require knowledge of previous events
      (count needs to know how many events it has seen so far in order to track the num ber of messages).




 +--------------------------------------------------------------------------------------------+
 | // Avro is a popular format in the Kafka community, largely due to its compact byte        |
 | // representation (which is advantageous for high throughput applications), native support |
 | // for record schemas, and a schema management tool called Schema Registry                 |
 | // There are other advantages, as well. For example, some Kafka Connectors can             |
 | // use Avro schemas to automatically infer the table structure of downstream data stores,  |
 | // so encoding our output records in this format can help with data integration downstream.|
 +--------------------------------------------------------------------------------------------+

in avro file: 

"namespace": "com.magicalpipelines.model",
# The desired package name for your data class.

"name": "EntitySentiment",
# The name of the Java class that will contain the Avro-based data model. 
# This class will be used in subsequent stream processing steps.




To support stateful operations, we need a way of storing and retrieving the remembered data, or state, 
    required by each stateful operator in our application (e.g. count, aggregate, join, etc). 
    The storage abstraction that addresses these needs in Kafka Streams is called a state store, 
    and since a single Kafka Streams application can leverage many stateful operators, 
    a single application may contain several state stores.


One important thing to look at when deciding between using a KTable or
    GlobalKTable is the keyspace. If the keyspace is very large (i.e. has high cardinality / lots
    of unique keys), or is expected to grow into a very large keyspace, then it
    makes more sense to use a KTable so that you can distribute fragments of the entire
    state across all of your running application instances. By partitioning the state in this
    way, we can lower the local storage overhead for each individual Kafka Streams instance.

Perhaps a more important consideration when choosing between a KTable or
    GlobalKTable is whether or not you need time synchronized processing. A
    KTable is time synchronized, so when Kafka Streams is reading from multiple
    sources (e.g. in the case of a join), it will look at the timestamp to determine which
    record to process next. This means a join will reflect what the combined record would
    have been at a certain time, and this makes the join behavior more predictable. On the
    other hand, GlobalKTables are not time synchronized, and are “completely populated before 
    any processing is done”.13 Therefore, joins are always made against the
    most up-to-date version of a GlobalKTable, which changes the semantics of the program.


A GlobalKTable should be used when your keyspace is small, you want to avoid the co-partitioning
requirements of a join and when time synchronization is not needed.


Kafka Streams includes three different join operators for joining streams and tables: 
   1- Inner join. The join is triggered when the input records 
      on both sides of the join share the same key.

   2- Left join. The join semantics are different depending on the type of join:
      a- for stream-stream joins - a join is triggered when a record on the left side of the join is
         received. If there is no record with the same key on the right side of the join, then the
         right value is set to null.
      b- for stream-stream and table-table joins - same semantics as a stream-stream left join
         (see above), except an input on the right side of the join can also trigger a lookup. If
         the right side triggers the join and there is no matching key on the left side, then the
         join will not produce a result.
    
   3- Outer join. The join is triggered when a record on the either side of the join 
      is received. If there is no matching record with the same key on the opposite 
      side of the join, then the corresponding value is set to null. 

To ensure related events are routed to the same partition, we must ensure the follow-
  ing co-partitioning requirements are met:
  1- Records on both sides must be keyed by the same field, and must be partitioned on that 
     key using the same partitioning strategy
  2- The input topics on both sides of the join must contain the same number of partitions 


When we add a key-changing operator to our topology, the underlying data will be marked
    for repartitioning. This means that as soon as we add a downstream operator that reads the
    new key, Kafka Streams will:
    send the rekeyed data to an internal repartition topic re-read the newly rekeyed data back 
    into Kafka Streams This process ensures related records (i.e. records that share the same key) 
    will be processed by the same task in subsequent topology steps. However, the network trip required
    for rerouting data to a special repartition topic means that rekey operations can be expensive.


 -- A message can have an optional piece of metadata, which is referred to as a key. The key is also 
       a byte array and, as with the message, has no specific meaning to Kafka. Keys are used when 
       messages are to be written to partitions in a more controlled manner. The simplest such 
       scheme is to generate a consistent hash of the key and then select the partition number for that
       message by taking the result of the hash modulo the total number of partitions in the topic.
       This ensures that messages with the same key are always written to the same partition 

-- Messages are written into Kafka in batches. A batch is just a collection
      of messages, all of which are being produced to the same topic and partition. An indi‐
      vidual round trip across the network for each message would result in excessive over‐
      head, and collecting messages together into a batch reduces this.


-- Messages in Kafka are categorized into topics. The closest analogies for a topic are a database table 
      or a folder in a filesystem. Topics are additionally broken down into a number of partitions

-- Partitions are also the way that Kafka provides redundancy and scalability. Each partition can 
      be hosted on a different server, which means that a single topic can be scaled horizontally 
      across multiple servers to provide performance far beyond the ability of a single serve
      Additionally, partitions can be replicated, such that different servers will store a 
      copy of the same partition in case one server fails.

-- By default, 
      the producer will balance messages over all partitions of a topic evenly. In some cases,
      the producer will direct messages to specific partitions. This is typically done using
      the message key and a partitioner that will generate a hash of the key and map it to a specific 
      partition. This ensures that all messages produced with a given key will get written to the same partition

-- The consumer keeps track of which messages it has already consumed by keeping track of
      the offset of messages. The offset—an integer value that continually increases is
      another piece of metadata that Kafka adds to each message as it is produced

-- Consumers work as part of a consumer group, which is one or more consumers that work together 
      to consume a topic. The group ensures that each partition is only con‐ sumed by one member.


-- A single Kafka server is called a broker. The broker receives messages from producers,
      assigns offsets to them  a single broker can easily handle thousands of partitions
      and millions of messages per second

-- Kafka brokers are designed to operate as part of a cluster. Within a cluster 
      of brokers, one broker will also function as the cluster controller 
      (elected automatically from the live members of the cluster). 

-- The controller is responsible for administrative operations, including assigning partitions 
      to brokers and monitoring for broker failures, A partition is owned by a single broker in the 
      cluster, and that broker is called the leader of the partition. A replicated partition is assigned to 
      additional brokers, called followers of the partition. Replication provides redundancy of messages 
      in the partition, such that one of the followers can take over leadership if there is a broker failure.

-- Kafka brokers are configured with a default retention setting for topics, either retaining 
     messages for some period of time (e.g., 7 days) or until the partition reaches a certain size 
     in bytes (e.g., 1 GB). Once these limits are reached, messages are expired and deleted
--  Individual topics can also be configured with their own retention settings so that messages are 
      stored for only as long as they are useful. For example, a tracking topic might be retained for several
      days, whereas application metrics might be retained for only a few hours. Topics can also be 
      configured as log compacted, which means that Kafka will retain only the last message produced with 
      a specific key. This can be useful for changelog-type data, where only the last update is interesting

-- The Kafka project includes a tool called MirrorMaker, used for replicating data to other
       clusters. At its core, MirrorMaker is simply a Kafka consumer and producer, linked together 
       with a queue. Messages are consumed from one Kafka cluster and produced to another.


-- Apache Kafka uses Apache ZooKeeper to store metadata about the Kafka cluster, as well as consumer 
      client details  ZooKeeper is a centralized service for maintaining configuration information, 
      naming, providing distributed synchronization, and providing group services. 


-- ZooKeeper is designed to work as a cluster, called an ensemble, to ensure high availa‐
      bility. Due to the balancing algorithm used, it is recommended that ensembles contain an odd number 
      of servers (e.g., 3, 5, and so on) as a majority of ensemble members (a quorum) must be working in 
      order for ZooKeeper to respond to requests. This means that in a three-node ensemble, you can run 
      with one node miss‐ ing. With a five-node ensemble, you can run with two nodes missing.


Kafka uses a configurable pool of threads for handling log segments. Currently, this thread pool is used:
    • When starting normally, to open each partition’s log segments
    • When starting after a failure, to check and truncate each partition’s log segments
    • When shutting down, to cleanly close log segments


-- Many users will have the partition count for a topic be equal to, or a multiple of, the number of brokers 
      in the cluster. This allows the partitions to be evenly distributed to the brokers, which will evenly
      distribute the message load. For example, a topic with 10 partitions operating in a Kafka cluster 
      with 10 hosts with leadership balanced among all 10 hosts will have optimal throughput. 


-- if we want to be able to write and read 1 GBps from a topic, and we know each consumer 
      can only process 50 MBps, then we know we need at least 20 partitions. This way,
      we can have 20 consumers reading from the topic and achieve 1 GBps

-- Kafka itself does not need much heap memory configured for the Java Virtual Machine 
       (JVM). Even a broker that is handling 150,000 messages per second and a data rate 
       of 200 megabits per second can run with a 5 GB heap. 


. A Kafka producer has three mandatory properties:
  bootstrap.servers
      List of host:port pairs of brokers that the producer will use to establish initial
      connection to the Kafka cluster. This list doesn’t need to include all brokers, since
      the producer will get more information after the initial connection. But it is rec‐
      ommended to include at least two, so in case one broker goes down, the producer
      will still be able to connect to the cluster.

  key.serializer
      Name of a class that will be used to serialize the keys of the records we will produce 
      to Kafka. Kafka brokers expect byte arrays as keys and values of messages.
      However, the producer interface allows, using parameterized types, any Java
      object to be sent as a key and value. This makes for very readable code, but it
      also means that the producer has to know how to convert these objects to byte
      arrays. key.serializer should be set to a name of a class that implements the
      org.apache.kafka.common.serialization.Serializer interface. The producer
      will use this class to serialize the key object to a byte array. The Kafka client package includes 
      ByteArraySerializer (which doesn’t do much), String Serializer, IntegerSerializer, and much more,
      so if you use common types, there is no need to implement your own serializers. Setting 
      key.serializer is required even if you intend to send only values, but you can use the Void 
      type for the key and the VoidSerializer.

  value.serializer
      Name of a class that will be used to serialize the values of the records we will produce to 
      Kafka. The same way you set key.serializer to a name of a class that
      will serialize the message key object to a byte array, you set value.serializer to
      a class that will serialize the message value object.


. There are three primary methods of sending messages
  Fire-and-forget
      We send a message to the server and don’t really care if it arrives successfully or not. Most 
      of the time, it will arrive successfully, since Kafka is highly available and the producer will 
      retry sending messages automatically. However, in case of nonretriable errors or timeout, messages 
      will get lost and the application will not get any information or exceptions about this.
      
  Synchronous send
      Technically, Kafka producer is always asynchronous—we send a message and the
      send() method returns a Future object. However, we use get() to wait on the
      Future and see if the send() was successful or not before sending the next record.
      
  Asynchronous send
      We call the send() method with a callback function, which gets triggered when it
      receives a response from the Kafka broker.     
